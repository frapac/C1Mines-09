\begin{frame}{Stochastic programs with recourse} 
Let's consider SP of the form
\[
\min_{x \in X}\, \E[f(x,\omega)]\,
\]
\pula
with
\begin{itemize}
\item $f:\Re^n\times \Omega \to \Re$ is convex on $x$ (decision variable)
\item $X \subset \R^n$ is a deterministic set (e.g. a fixed polyhedron)
\item $\omega$ is a random vector and $(\Omega,\mathcal{F},P)$ is its probability space
\item $\E[\cdot]$ is the expected value w.r.t. the probability measure $P$
\end{itemize}
\pula
For instance, in a two-stage stochastic linear framework, we have
\[
f(x,\omega) = c^\top x + Q(x,\omega)
\]
with
\[Q(\verm{x},\azul{\omega}):=\left\{ \begin{array}{rll}
\min & q(\azul{\omega})^\top y &\\
\mbox{s.t.} & W(\azul{\omega}) y   =h(\azul{\omega})-T(\azul{\omega})\verm{x}&\\ 
& y  \geq 0  &
\end{array} \right.\]
\end{frame}


%\begin{frame}{Example} 
%{
%\[\hspace{-1cm}(P)\quad \left\{ \begin{array}{cl}\displaystyle \min_{x,y(\omega)} & 2x_{1} + 3x_{2} +\E[ 7y_1(\omega) + 12y_2(\omega)]\\
%& \begin{array}{rrrll}
%x_{1} & + x_{2} &  &  \leq 100&\\
%2x_{1}  &+ 6x_{2} &+y_1(\omega) &  \geq h_1(\omega)\\
%3x_{1}  &+ 3x_{2} &  +y_2(\omega) & \geq h_2(\omega)\\
% & x_{1} \,, x_{2}\geq0\,,& y_1(\omega) \,, y_2(\omega)  & \geq 0  
%\end{array}
%\end{array}\right.\]
% }
%\verm{The studied example of oil/gasoline management fits this formulation}
% \pula
% 
%Define $f(x,\verm{\omega}) = 2x_{1} + 3x_{2} + Q(x,\verm{\omega})$, with 
%\[Q(x,\verm{\omega}):=\left\{
%\begin{array}{lllll}
%\displaystyle \min_{y\geq 0} &7y_1 + 12y_2\\
%\mbox{s.t.}
%&y_1   \geq h_1(\verm{\omega}) -(2x_{1}  + 6x_{2} )\\
%&  y_2 \geq h_2(\verm{\omega})-(3x_{1}  + 3x_{2} )
%\end{array}\right.
%\]
%
%which is equivalent to 
%\[\azul{Q(x,\omega)=
%7[h_1(\omega) -(2x_{1}  + 6x_{2} )]^+ + 12[ h_2(\omega)-(3x_{1}  + 3x_{2} )]^+
%}
%\]
%Therefore, (P) can be written as
%\[
%\min_{x} \E[f(x,\omega)]\quad \mbox{s.t.}\quad x \in X:=\{x_1,x_2\geq0, \; x_1+x_2 \leq 100\}
%\]
%\pula
%
%\azul{W.l.o.g. we stick with the generic formulation of SP with recourse:
%\[
%\min_{x \in X}\, \E[f(x,\omega)]
%\]
%}
%\end{frame}


\begin{frame}{Representation of the uncertainties}
\begin{block}{Continuous probability distribution}
\begin{itemize}
\item Sample space  $\Omega$ contains infinitely many elements
\[
\min_{x \in X}\, \E[f(x,\omega)]
\]

\item  For computational reasons, it is necessary to  consider finitely many scenarios $\omega^i \in \Omega$, with associated probability $p_i>0$
\pula

\item Resulting problem
\[ 
\min_{x \in X}\, f^N(x)\quad \mbox{with}\quad f^N(x):=\sum_{i=1}^{N}p_if(x,\omega^i)
\]
\end{itemize}
\end{block}
\begin{block}{Sample Average Approximation - SAA}
\[
\min_{x \in X}\, \frac{1}{N}\sum_{i=1}^{N}f(x,\omega^i)
\]
\end{block}
 \end{frame}
 
 
 
\begin{frame}
\begin{block}{How to proceed when we do not know $P$?}
\begin{itemize}
\item In many applications the probability distribution is not precisely known
\pula

\item In these cases, $ P $ is estimated by using the historical of the stochastic vector
\pula

\item Scenario generation can be done via Monte Carlo simulation (it is advisable not to use
the historical as scenarios)
\end{itemize}
\pula
\end{block}
\begin{block}{Representation of the uncertainties}
\begin{center}
\includegraphics[width=5cm]{../Figs/formatos.pdf} {}
\end{center}
\end{block} 
\end{frame}





\begin{frame}{The newsvendor problem}

\begin{itemize}


\item A newsvendor buys newspaper  by the morning at price \azul{$c$} and sells them along the day at price \azul{$r$}
\pula

\item Unsold newspaper are sent to be recycled. The value earned by every recycled newspaper is \azul{$s$}
\pula

\item The newsvendor wishes to maximize its expected income:
\[
\min_{x\geq 0} \E[f(x,\omega)],
\]
where
\[
\begin{array}{lll}
f(x,\omega) &= -[-cx + r\min\{x,\omega\} +  s(x - \min\{x,\omega\})]\\
&=
-[(s-c)x + (r-s)\min\{x,\omega\}]
\end{array}
\]
\end{itemize}

 \end{frame} 





%\begin{frame}{Sampling}

%\begin{itemize}

%\item For every given $x$ we could think of approximating $f(x)=\E[f(x,\omega)]$  by a sample average
%\pula


%\item I.e., for every given $x \in X$ we can sample $\{\omega_x^1,\ldots, \omega_x^N\}$ and approximate $f(x)$
%by
%\[
%f^N(x) = \frac{1}{N}\sum_{i=1}^N f(x,\omega_x^i)
%\]
%\pula
%\pause
%\item Generating a different sample for each $x$ is useless:
%\end{itemize}
%\begin{center}
%\includegraphics[width=6cm]{Figs/jornaleiro1.png} {}
%\end{center}
%\end{frame} 
 
 

\begin{frame}{Sample Average Approximation - SAA}

\begin{itemize}
\item The main idea of the \azul{Sample Average Approximation} - SAA - approach is to use the same sample for all $x \in X$
\pula

\item I.e., we draw a sample $\{\omega^1,\ldots, \omega^N\}$ and approximate $f(x)$ by
\[
f^N(x) = \frac{1}{N}\sum_{i=1}^N f(x,\omega^i)
\]
regardless the given point $x$
\end{itemize}
\begin{center}
\includegraphics[width=6.5cm]{../Figs/jornaleiro2.png} {}
\end{center}
 \end{frame} 




\begin{frame}{Sample Average Approximation - SAA}

\begin{itemize}

\item The approximation of $f^N$ of $f$ is quite close to $ f $
\pula

\item This suggests replacing the original problem $ \min_{x \in X} f (x) $ by
\[
\min_{x \in X} f^N (x) 
\]
which can be solved by deterministic methods (L-Shaped, Nested Decomposition, Bundle Method, etc.)
\end{itemize}
\begin{block}{Questions}
\begin{itemize}
\item Does the SAA approach always work regardless the function $f(x,\omega)$?
\pula

\item What is a good size $N$ of the sample to be considered?
\pula

\item What can we say about the quality of the SAA solution?
\end{itemize}
\end{block}

 \end{frame} 
 
 
 

\begin{frame}{Asymptotic properties}

Let 
\begin{itemize}
\item $\hat x^N$ be a solution of the SAA problem
\item $\hat S^N$ be the solution set of the SAA problem
\item $\hat f^N$ be the optimal value of the SAA problem
\end{itemize}
and
\begin{itemize}
\item $ x^*$ be a solution of the true problem
\item $S^*$ be the solution set of the true problem
\item $f^*$ be the optimal value of the true problem
\end{itemize}


\begin{block}{Questions}
\begin{itemize}
\item $\lim_{N\to \infty} \hat x^N = x^*$?
\pula 
\item $\lim_{N\to \infty} dist(S^N,S^*) = 0$?
\pula
\item $\lim_{N\to \infty} \hat f^N = f^*$?

\end{itemize}
\end{block}
 \end{frame} 
 
 
 
\begin{frame}{Asymptotic properties}
\begin{itemize}
\item Firstly, let's try to answer these questions by considering the newsvendor problem
\pula

\item Suppose the demand for newspaper follows an Exponential probability distribution
\[
\omega \sim {\tt Exponencial}(10),
\quad \mathbb{P}[\omega \leq x]= 1 - e^{-10x}\quad \mbox{(if $x\geq 0$)}
\]
\end{itemize}
\begin{center}
\includegraphics[width=9cm]{../Figs/jornaleiro3.png} {}
\end{center}
\end{frame} 





\begin{frame}{Asymptotic properties}

\begin{itemize}

\item The table presents the values of $\hat x^N$ and $\hat f^N$ for some different sample size $N$

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
N & 10 & 30 & 90 & 270 & $\infty$ \\ 
\hline 
$\hat x^N$ & 1.46 & 1.44 & 1.54 & 2.02 & 2.23 \\ 
\hline 
$\hat f^N$ & -1.11 & -0.84 & -0.98 & -1.06 & -1.07 \\ 
\hline 
\end{tabular} 
\end{center}
\item It seems that $f^N$ approximates well $f^*$ when $N$ increases
\pula

\item Notice that $\hat x^N \to x^*$ and $\hat f^N \to f^*$
\end{itemize}
 \end{frame} 
 
 

\begin{frame}{Asymptotic properties}

\begin{itemize}
\item Suppose now that demand for newspaper follows a discrete uniform probability distribution on the set
\[
\{1,3,\ldots, 10\}\,
\]
\end{itemize}
\begin{center}
\includegraphics[width=9cm]{../Figs/jornaleiro4.png} {}
\end{center}
\end{frame} 
 
 
\begin{frame}{Asymptotic properties}

\begin{itemize}

\item The table presents the values of $\hat x^N$ and $\hat f^N$ for some different sample size $N$


\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
N & 10 & 30 & 90 & 270 & $\infty$ \\ 
\hline 
$\hat x^N$ & 2 & 3 & 3 & 2 & [2,3] \\ 
\hline 
$\hat f^N$ & -2.00 & -2.50 & -1.67 & -1.35 & -1.50 \\ 
\hline 
\end{tabular} 
\end{center}

\item Again, it seems that  $f^N$ approximates well $f^*$ when $N$ increases
\pula

\item Notice that  $\hat f^N \to f^*$, but $\hat x^N$ does not seem to converge 
\pula
\item $\hat x^N$ is oscillating between two optimal solutions of the problem
\pula

\item What can we conclude?
\end{itemize}

 \end{frame} 

\begin{frame}{Convergence results}



\begin{itemize}
\item In both cases (continuous and discrete) the function  $f^N$ converges uniformly to $f$
\pula

\item Uniform convergence occurs, for instance, when $f$ is continuous 
\pula

\item When continuous convergence is observed, we have the following results
\end{itemize}

\begin{theorem}

\begin{itemize}
\item $\lim_{N \to \infty} \hat f^N = f^*$ with probability 1 (w.p.1)
\pula
\item Suppose there exists a compact set $C$ such that
\begin{itemize}
\item $\emptyset\neq S^* \subset  C$ and $\emptyset\neq S^N \subset  C$ (w.p.1) for $N$ large enough
\item the objective function is continuous and finite-valued on $C$ 
\end{itemize}
\pula
\verm{Then $\lim_{N \to \infty} dist(S^N,S^*) = 0$ w.p.1}
\end{itemize}
\end{theorem}
 \end{frame} 


\begin{frame}{What does ``convergence w.p.1" mean?}

\begin{itemize}
\item  Each function  $f^N$ is constructed with a single sample
\pula
\item Regardless the sample,  convergence results hold provided that $N \to \infty$ 
\end{itemize}
\pula
Let's repeat the same experiment for $N=270$ several times:


\begin{center}
\includegraphics[width=12cm]{../Figs/jornaleiro5.png} {}
{\small
(a) Exponential distribution \quad \quad (b) Discrete uniform distribution}
\end{center}
 \end{frame} 
 
 
 
\begin{frame}{What does ``convergence w.p.1" mean?}


\begin{itemize}
\item For some samples with $N=270$ the approximation is quite good. However, for other samples the approximation is poor
\pula

\item Why don't we have convergence for every sample?
\pula
\pause

\item \verm{The theorem only ensures convergence when $N \to \infty$...}
\end{itemize}

\pula
\azul{Given a sample of size of $ N $, we solve the SAA problem $\min_{x \in X} f^N (x) $
\begin{center}
\shadowbox{How do we know if we have a ``good'' or ``bad '' sample?}
\end{center}
}

\pause
\begin{itemize}
\item The answer is: \verm{we do not know for sure}. However, we may employ simulation and statistical tools to access quality
\end{itemize}

 \end{frame} 


\begin{frame}{A result on the sample size}

\[
 \min_{x \in X} f(x)=\E[f(x,\omega)] \quad \quad (SAA) \quad \min_{x \in X} f^N(x)=\frac{\sum_{i=1}^N f(x,\omega^i)}{N}
\]


\begin{theorem}
Suppose the true stochastic program has a unique solution $x^*$, $X$ is a compact set and $f(\cdot, \omega)$ is strongly convex. 
Then, for all $\epsilon >0$ there exist constants $\beta(\epsilon)>0$ and $C>0$ such that
\[
\mathbb{P}[\|\hat x^N- x^*\|>\epsilon] \leq  C e^{-N\,\beta(\epsilon)}
\]

\pause 

\azul{The theorem ensures the existence of such constants, but not their values}
\pula

\verm{We need to perform simulation...}
\end{theorem}

 \end{frame} 
  
  
  
\begin{frame}{Simulation}

\begin{itemize}
\item Given a sample $\{\omega^1,\ldots,\omega^N\}$, define
\[
\hat x^N \in \arg \min_{x \in X} f^N(x), \quad \mbox{and}\quad f^N(x)=\frac{\sum_{i=1}^N f(x,\omega^i)}{N}\quad (SAA)
\]
\pula
\item In order to assess the quality of $\hat x^N$ it is mandatory to generate a  larger sample $\{\tilde \omega^1,\ldots,\tilde \omega^{N'}\}$ independent of $\{\omega^1,\ldots,\omega^N\}$ and evaluate the costs
\[
f(\hat x^N, \tilde \omega^j), \quad j=1,\ldots N' \;\; (>> N)
\]
(\azul{Evaluating the function is easier than solving the SAA problem})
\end{itemize}

 \end{frame} 



\begin{frame}{Simulation}
\[
\hat x^N \in \arg \min_{x \in X} f^N(x), \quad \quad f^N(x)=\frac{\sum_{i=1}^N f(x,\omega^i)}{N}\quad (SAA)
\]

\begin{itemize}
\item In order to infer if the sample size $N$  is satisfactory we may compare
\pula
\begin{itemize}
\item $f^N(\hat x^N)$ with $f^{N'}(\hat x^N)$   (average of the individual costs $f(\hat x^N, \tilde \omega^j)$)
\pula

\item Empirical distribution of the individual costs  $f(\hat x^N, \tilde \omega^j),\;\;j=1,\ldots,N'$ and $f(\hat x^N,\omega^i),\;\;i=1,\ldots,N$ (KS-test)
\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=5cm]{../Figs/KSteste.png} {}
\end{center}
 \end{frame} 
 
\begin{frame}{Simulation}

Another idea widely used in practice is:
\begin {itemize}
\item Given $ N $ and $ M $, generate $ M $  samples of size $ N $ and solve $ M $ problems $ SAA $
\pula

\item Compare the most important variables of the $ M $  SAA solutions $ \hat x^N_i$, $ i = 1, \ldots, M$
\pula

\item Evaluate the SAA solutions using a larger sample $\{ \tilde \omega^1, \ldots, \tilde \omega^{N'} \}$
\pula

\item Compare the empirical cost distributions
\end{itemize}
\pula

\azul{If there is a certain ``adherence" among the results, the size $ N $ can be considered satisfactory. Otherwise, it is suggested to increase $ N $}

\pula
\verde{Importance of simulation}
\begin{itemize}
\item It allows  us to analyze the quality of solution obtained with the stochastic model
\pula

\item it allows us to estimate an appropriated sample size
\end{itemize}
\end{frame} 

%%%%%%%%%%%%%%%

\begin{frame}{Computing confidence intervals}
\verm{Let $x$ be fixed }(for instance, $x=x^N$ the solution of the SAA model)

\pula

\azul{The strong law of large numbers} ensures, for $N$ large enough
\[
f^N(x):=\frac{1}{N}\sum_{i=1}^N f(x,\omega^i) \approx \int_{\Omega}f(x,\omega)dP(\omega)=\E[f(x,\omega)]:=f(x)
\]
w.p.1, provided that $\{\omega^1,\ldots, \omega^N\}$ is a iid sample of $\omega$ and $\E[\omega]$ is finite (no assumption on the distribution of $\omega$ is required!)
\pula

Thus, we can use $f^N(x)$ as an approximation of \[f(x)=\E[f(x,\omega)]\]

\begin{block}{Difficulties}
\begin{itemize}
\item $f^N(x)$ is a random variable itself: it depends on the sample $\{\omega^1,\ldots, \omega^N\}$
\pula

\item Sometimes $f^N$ can be an accurate approximation of $f$, sometimes not
\end{itemize}
\end{block}
 \end{frame} 

\begin{frame}{The Central Limit Theorem }
 As $f^N$ is a random variable, it makes sense to compute its mean and variance. Let $x$ be a given point:
\[
\E[f^N(x)] = \E[\frac{1}{N}\sum_{i=1}^N f(x,\omega^i)]= \frac{1}{N}\sum_{i=1}^N \E[f(x,\omega^i)] =f(x)
\]
\azul{Therefore, $f^N(x)$ is an unbiased estimator of $f(x)$}

\pula
\[
\begin{array}{ll}
\var[f^N(x)]&= \var[\frac{1}{N}\sum_{i=1}^N f(x,\omega^i)]=  \frac{1}{N^2}\sum_{i=1}^N \var[f(x,\omega^i)]= \frac{1}{N} \var[f(x,\omega)]
\end{array}
\]
\verm{ Thus, the variance of $f^N(x)$ vanishes when $N$ goes to infinity} (if $\sigma^2=\var[f(x,\omega)]$ is finite, of course)


\begin{block}{The Central Limit Theorem - CLT}
\[
\frac{\sqrt{N}[f^N(x) - f(x)]}{\sigma}\approx \mathcal{N}(0,1)
\]
\end{block}
 \end{frame} 
 

\begin{frame}{The Central Limit Theorem }
\[
\frac{\sqrt{N}[f^N(x) - f(x)]}{\sigma}\approx \mathcal{N}(0,1)
\]
\pula 

The CLT ensures that, for an arbitrary $x \in {\tt dom}\, (f)$, $\sqrt{N}[f^N(x) - f(x)]$
converges in distribution to normal distribution with zero mean and variance equal to $\sigma^2=\var[f(x,\omega)]$ (provided that this variance is finite)
\pula

Consequently $f^N(x)$ converges to $f(x)$ at a (stochastic) rate of $\mathcal{O}_1(N^{-1/2})$
\pula

In other words, in order to estimate $f(x)$ by its sample average with an
accuracy $\epsilon>0$ one needs a sample of size $N = \mathcal{O}_2(\epsilon^{-2})$
\pula 

Although various techniques, e.g., variance reduction techniques and quasi-Monte Carlo
methods, were developed in simulation literature in order to enhance the
accuracy of such estimates,  it is basically impossible to evaluate multivariate integrals with
a high precision
\pula

As a conclusion, when solving a SAA problem, keep in mind that you're solving only an (possibly rough) approximation of the ``true" stochastic program

\pula
However, there are manners to estimate how good or how bad is the SAA solution...

 \end{frame} 

\begin{frame}{Computing the error of the estimated value}
{Confidence interval}

The CLT ensures that
\[
P\left[f^N(x) - 1.96\frac{\sigma}{\sqrt{N}} \leq f(x) \leq f^N(x) + 1.96\frac{\sigma}{\sqrt{N}}\right]= 0.95
\]
\pula

In practical terms, the above expression  means that for each 100 samples $\{\omega^1,\ldots,\omega^N\}$, the true value $f(x)$ is contained in 95 intervals
\[
\left[f^N(x) - 1.96\frac{\sigma}{\sqrt{N}} ,\, f^N(x) + 1.96\frac{\sigma}{\sqrt{N}}\right]
\]
\pula

The variance  $\sigma^2$ is generally unknown, but it can be estimated by
\[
S^2 = \frac{\sum_{i=1}^N [f(x,\omega^i) - f^N(x)]^2}{N-1}
\]
 \end{frame} 


\begin{frame}{ }
The true stochastic optimization problem can be solved by
the SAA approach in a reasonable time with a reasonable accuracy provided
that the following conditions are satisfied:
\begin{itemize}
\item  (i) the required sample size is
manageable
\item  (ii) it is possible to solve the constructed SAA problem with
a reasonable efficiency
\end{itemize} 
\pula 

From this point of view the number of scenarios of
the true problem (i.e., cardinality of the support $\Omega$ of the distribution of $\omega$)
is irrelevant and can be infinite
\pula

Condition (ii) holds in the case of two
stage linear stochastic programming with recourse
%\pula

%Condition (i) holds if for every feasible $x$ variability of $f(x,\omega)$ is ``not too large"
%(in particular,
%$f(x,\omega)$ should be finite valued for a.e. realization of $\omega$
\end{frame}
  
  
\begin{frame} {Some conclusions} 
\begin{itemize}

\item Stochastic optimization decisions are in general balanced: protect against ``bad" scenarios
\pula

%\item Solutions depend (strongly) on the considered scenarios
%\pula

\item The complexity of the optimization problem grows with the number of
scenarios
\pula

\item A SAA problem is frequently used to approximate a stochastic program
\pula

\item Once a SAA solution is determined, it is crucial to evaluate the quality of the output through simulations
\end{itemize}

\end{frame}
%=======================================================
